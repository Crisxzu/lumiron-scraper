# LumironScraper Backend

API REST Flask pour Due Diligence OSINT - Analyse compl√®te de profils professionnels avec donn√©es officielles fran√ßaises.

## üéØ Fonctionnalit√©s

- **Due Diligence v3** - 18 sections enrichies (vs 11 en v2) : psychologie, finances, r√©seau, analyse juridique
- **Sources officielles** - Pappers (l√©gal/financier), DVF (immobilier), HATVP (PPE)
- **Streaming SSE** - Suivi temps r√©el avec 6 √©tapes de progression (~2-3min)
- **Double s√©curit√©** - Prompt renforc√© + validation Python post-LLM (anti-hallucination)
- **Scraping multi-sources** - Architecture modulaire (Serper, Firecrawl, sites entreprises)
- **Analyse LLM** - GPT-4o avec temp√©rature 0.3 pour pr√©cision maximale
- **Cache SQLite** - Expiration configurable + force refresh
- **Prompts Jinja2** - Templates √©ditables avec exemples inline

## üîß Pr√©requis

- **Python 3.12.6**
- pip + venv

## üì¶ Installation

```bash
# Environnement virtuel
python3.12 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# D√©pendances
pip install -r requirements.txt
```

## ‚öôÔ∏è Configuration

```bash
# Copier .env.example
cp .env.example .env

# √âditer et configurer les cl√©s API
nano .env
```

### Variables essentielles

```bash
# APIs (obligatoires)
OPENAI_API_KEY=sk-...
FIRECRAWL_API_KEY=fc-...
SERPER_API_KEY=...
PAPPERS_API_KEY=...      # Donn√©es l√©gales/financi√®res
PAPPERS_MODE=standard    # ou 'complete' (mode √©tendu)

# Flask
PORT=5100
CORS_ORIGINS=http://localhost:5101,http://localhost:5173

# Cache
CACHE_TTL_SECONDS=604800  # 7 jours
DATABASE_PATH=data/lumironscraper.db

# Scraping
MAX_TOTAL_SCRAPES=15     # 15 scrapes pour v3 enrichi
OPENAI_MODEL=gpt-4o
```

### Obtenir les cl√©s API

- **OpenAI**: https://platform.openai.com/api-keys (~$0.04/profil en v3)
- **Firecrawl**: https://firecrawl.dev ($0.003/page)
- **Serper**: https://serper.dev (2500 gratuites, puis $0.005/recherche)
- **Pappers**: https://www.pappers.fr/api (20‚Ç¨/mois = 1000 cr√©dits, ~5-10 cr√©dits/profil)

## üöÄ D√©marrage

### D√©veloppement

```bash
python main.py
# ‚Üí http://localhost:5100
```

### Production

```bash
gunicorn --bind 0.0.0.0:5100 --workers 4 "app:create_app()"
```

### Docker

```bash
# Build
docker build -t lumironscraper-backend .

# Run
docker run -p 5100:5100 --env-file .env lumironscraper-backend
```

## üì° API Endpoints

### Recherche classique

```bash
POST /api/v1/search
Content-Type: application/json

{
  "first_name": "Anthony",
  "last_name": "Tartour",
  "company": "Lumiron",
  "force_refresh": false
}
```

**R√©ponse (v3 - 18 sections):**
```json
{
  "success": true,
  "cached": false,
  "data": {
    "full_name": "Anthony Tartour",
    "current_position": "Co-Founder",
    "company": "Lumiron",
    "credibility_score": 75,
    "reputation_score": 80,
    "influence_score": 65,
    "reliability_score": 70,
    "risk_level": "Moyen",
    "professional_experience": [...],
    "business_ecosystem": {...},
    "financial_intelligence": {...},
    "psychology_and_approach": {...},
    "red_flags": [...],
    // + 13 autres sections
  }
}
```

### Recherche avec streaming SSE (recommand√©)

```bash
POST /api/v1/search-stream
Content-Type: application/json

{
  "first_name": "Anthony",
  "last_name": "Tartour",
  "company": "Lumiron"
}
```

**R√©ponse (Server-Sent Events):**
```
data: {"type":"progress","step":"cache","percent":5,"message":"V√©rification du cache..."}

data: {"type":"progress","step":"pappers","percent":15,"message":"R√©cup√©ration donn√©es Pappers..."}

data: {"type":"progress","step":"dvf","percent":25,"message":"Recherche DVF (immobilier)..."}

data: {"type":"progress","step":"hatvp","percent":35,"message":"V√©rification HATVP (PPE)..."}

data: {"type":"progress","step":"scraping","percent":50,"message":"Scraping des pages (15 scrapes, ~2min)..."}

data: {"type":"progress","step":"analysis","percent":85,"message":"Analyse GPT-4o (enrichissement)..."}

data: {"type":"complete","data":{...}}
```

### Autres endpoints

```bash
GET  /api/v1/health           # Health check
GET  /api/v1/cache/stats      # Stats du cache
POST /api/v1/cache/clear-expired  # Nettoyage
```

## üèóÔ∏è Architecture

```
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # Factory Flask
‚îÇ   ‚îú‚îÄ‚îÄ db/                   # SQLite setup
‚îÇ   ‚îú‚îÄ‚îÄ models/               # Pydantic schemas v3 (18 sections)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ person_profile.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ person_profile_v3.py
‚îÇ   ‚îú‚îÄ‚îÄ routes/               # API routes (search, search-stream)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py      # + validation post-LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper_service.py
‚îÇ   ‚îú‚îÄ‚îÄ sources/              # Sources modulaires
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_source.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ serper_search_source.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pappers_source.py    # Donn√©es l√©gales FR
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dvf_source.py        # Immobilier FR
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hatvp_source.py      # PPE FR
‚îÇ   ‚îú‚îÄ‚îÄ templates/prompts/    # Prompts Jinja2 + exemples inline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ due_diligence_analysis.txt  # Prompt v3
‚îÇ   ‚îî‚îÄ‚îÄ utils/                # Utilitaires
‚îú‚îÄ‚îÄ data/                     # SQLite DB (auto-cr√©√©)
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ .env.example
```

## üîå Ajouter une Source

### 1. Cr√©er le fichier

```python
# app/sources/my_source.py

from typing import List
from app.sources.base_source import BaseSource

class MySource(BaseSource):
    @classmethod
    def get_name(cls) -> str:
        return "my_source"

    @classmethod
    def get_description(cls) -> str:
        return "Ma source custom"

    def get_urls(self, first_name: str, last_name: str, company: str) -> List[str]:
        # Logique de g√©n√©ration d'URLs
        return [
            f"https://example.com/{first_name}-{last_name}",
            # ...
        ]
```

### 2. Enregistrer

```python
# app/sources/__init__.py

from app.sources.my_source import MySource

AVAILABLE_SOURCES = [
    SerperSearchSource,
    CompanyWebsiteSource,
    MySource,  # ‚Üê Ajouter ici
]
```

### 3. Red√©marrer

```bash
python main.py
```

C'est tout ! La source sera automatiquement int√©gr√©e.

## üíæ Cache SQLite

Le cache stocke les profils dans `data/lumironscraper.db` avec:
- **Expiration** bas√©e sur `CACHE_TTL_SECONDS`
- **Force refresh** via param√®tre API
- **Compteur d'acc√®s** pour analytics

```bash
# Backup
cp data/lumironscraper.db backups/lumironscraper_$(date +%Y%m%d).db

# Compacter
sqlite3 data/lumironscraper.db "VACUUM;"
```

## üö¢ D√©ploiement

### Heroku

```bash
heroku create lumironscraper-backend
heroku config:set OPENAI_API_KEY=sk-...
heroku config:set FIRECRAWL_API_KEY=fc-...
heroku config:set SERPER_API_KEY=...
git push heroku main
```

### Railway

1. Connecter le repo GitHub
2. Ajouter les variables d'environnement
3. D√©ploiement automatique via `Procfile`

### VPS

```bash
sudo apt install python3.12 python3.12-venv
git clone https://github.com/Crisxzu/lumiron-scraper.git
cd LumironScraper/backend
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
nano .env
gunicorn --bind 0.0.0.0:5100 --workers 4 "app:create_app()"
```

## üõ†Ô∏è D√©veloppement

### Modifier le prompt

```bash
nano app/templates/prompts/profile_analysis.txt
```

Red√©marrer pour appliquer les changements.

### Logs

```python
print(f"[Service] ‚úì Success")
print(f"[Service] ‚úó Error")
print(f"[Service] ‚ö† Warning")
```

### Tests

```bash
# Health check
curl http://localhost:5100/api/v1/health

# Search
curl -X POST http://localhost:5100/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{"first_name":"Satya","last_name":"Nadella","company":"Microsoft"}'
```
