# LumironScraper Backend

API REST Flask pour scraping et analyse intelligente de profils professionnels.

## üéØ Fonctionnalit√©s

- **Scraping multi-sources** - Architecture modulaire (Serper, sites entreprises, LinkedIn)
- **Analyse LLM** - OpenAI GPT-4 pour structurer les donn√©es en fran√ßais
- **Cache SQLite** - Expiration configurable + force refresh
- **Fusion LinkedIn** - Combine tous les profils trouv√©s
- **Prompts Jinja2** - Templates √©ditables

## üîß Pr√©requis

- **Python 3.12.6**
- pip + venv

## üì¶ Installation

```bash
# Environnement virtuel
python3.12 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# D√©pendances
pip install -r requirements.txt
```

## ‚öôÔ∏è Configuration

```bash
# Copier .env.example
cp .env.example .env

# √âditer et configurer les cl√©s API
nano .env
```

### Variables essentielles

```bash
# APIs (obligatoires)
OPENAI_API_KEY=sk-...
FIRECRAWL_API_KEY=fc-...
SERPER_API_KEY=...

# Flask
PORT=5100
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Cache
CACHE_TTL_SECONDS=604800  # 7 jours
DATABASE_PATH=data/lumironscraper.db

# Scraping
MAX_TOTAL_SCRAPES=3
OPENAI_MODEL=gpt-4o
```

### Obtenir les cl√©s API

- **OpenAI**: https://platform.openai.com/api-keys (~$0.01/profil)
- **Firecrawl**: https://firecrawl.dev ($0.003/page)
- **Serper**: https://serper.dev (2500 recherches gratuites, puis $0.005/recherche)

## üöÄ D√©marrage

### D√©veloppement

```bash
python main.py
# ‚Üí http://localhost:5100
```

### Production

```bash
gunicorn --bind 0.0.0.0:5100 --workers 4 "app:create_app()"
```

### Docker

```bash
# Build
docker build -t lumironscraper-backend .

# Run
docker run -p 5100:5100 --env-file .env lumironscraper-backend
```

## üì° API Endpoints

### Recherche de profil

```bash
POST /api/v1/search
Content-Type: application/json

{
  "first_name": "Satya",
  "last_name": "Nadella",
  "company": "Microsoft",
  "force_refresh": false  // Optionnel
}
```

**R√©ponse:**
```json
{
  "success": true,
  "cached": true,
  "cache_age_seconds": 3600,
  "data": {
    "full_name": "Satya Nadella",
    "current_position": "Directeur G√©n√©ral",
    "company": "Microsoft",
    "professional_experience": [...],
    "skills": [...],
    "summary": "...",
    "sources": [...]
  }
}
```

### Autres endpoints

```bash
GET  /api/v1/health           # Health check
GET  /api/v1/cache/stats      # Stats du cache
POST /api/v1/cache/clear-expired  # Nettoyage
```

## üèóÔ∏è Architecture

```
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # Factory Flask
‚îÇ   ‚îú‚îÄ‚îÄ db/                   # SQLite setup
‚îÇ   ‚îú‚îÄ‚îÄ models/               # Pydantic schemas
‚îÇ   ‚îú‚îÄ‚îÄ routes/               # API routes
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper_service.py
‚îÇ   ‚îú‚îÄ‚îÄ sources/              # Sources modulaires
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_source.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ serper_search_source.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ company_website_source.py
‚îÇ   ‚îú‚îÄ‚îÄ templates/prompts/    # Templates Jinja2
‚îÇ   ‚îî‚îÄ‚îÄ utils/                # Utilitaires
‚îú‚îÄ‚îÄ data/                     # SQLite DB (auto-cr√©√©)
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ .env.example
```

## üîå Ajouter une Source

### 1. Cr√©er le fichier

```python
# app/sources/my_source.py

from typing import List
from app.sources.base_source import BaseSource

class MySource(BaseSource):
    @classmethod
    def get_name(cls) -> str:
        return "my_source"

    @classmethod
    def get_description(cls) -> str:
        return "Ma source custom"

    def get_urls(self, first_name: str, last_name: str, company: str) -> List[str]:
        # Logique de g√©n√©ration d'URLs
        return [
            f"https://example.com/{first_name}-{last_name}",
            # ...
        ]
```

### 2. Enregistrer

```python
# app/sources/__init__.py

from app.sources.my_source import MySource

AVAILABLE_SOURCES = [
    SerperSearchSource,
    CompanyWebsiteSource,
    MySource,  # ‚Üê Ajouter ici
]
```

### 3. Red√©marrer

```bash
python main.py
```

C'est tout ! La source sera automatiquement int√©gr√©e.

## üíæ Cache SQLite

Le cache stocke les profils dans `data/lumironscraper.db` avec:
- **Expiration** bas√©e sur `CACHE_TTL_SECONDS`
- **Force refresh** via param√®tre API
- **Compteur d'acc√®s** pour analytics

```bash
# Backup
cp data/lumironscraper.db backups/lumironscraper_$(date +%Y%m%d).db

# Compacter
sqlite3 data/lumironscraper.db "VACUUM;"
```

## üö¢ D√©ploiement

### Heroku

```bash
heroku create lumironscraper-backend
heroku config:set OPENAI_API_KEY=sk-...
heroku config:set FIRECRAWL_API_KEY=fc-...
heroku config:set SERPER_API_KEY=...
git push heroku main
```

### Railway

1. Connecter le repo GitHub
2. Ajouter les variables d'environnement
3. D√©ploiement automatique via `Procfile`

### VPS

```bash
sudo apt install python3.12 python3.12-venv
git clone https://github.com/your-repo/LumironScraper.git
cd LumironScraper/backend
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
nano .env
gunicorn --bind 0.0.0.0:5100 --workers 4 "app:create_app()"
```

## üõ†Ô∏è D√©veloppement

### Modifier le prompt

```bash
nano app/templates/prompts/profile_analysis.txt
```

Red√©marrer pour appliquer les changements.

### Logs

```python
print(f"[Service] ‚úì Success")
print(f"[Service] ‚úó Error")
print(f"[Service] ‚ö† Warning")
```

### Tests

```bash
# Health check
curl http://localhost:5100/api/v1/health

# Search
curl -X POST http://localhost:5100/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{"first_name":"Satya","last_name":"Nadella","company":"Microsoft"}'
```
