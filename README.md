# ğŸ” LumironScraper

**Due Diligence OSINT - Intelligence Ã©conomique et analyse de risque**

Solution full-stack pour l'analyse complÃ¨te de profils professionnels avec donnÃ©es officielles franÃ§aises. <br/>
Ã€ partir d'un prÃ©nom, nom et entreprise, le systÃ¨me collecte des informations publiques (web + bases lÃ©gales) et utilise GPT-4o pour gÃ©nÃ©rer une analyse Due Diligence enrichie en 18 sections avec scoring de risque.

---

## ğŸ“‹ Table des matiÃ¨res

- [Stack Technique](#-stack-technique)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [DÃ©marrage Rapide](#-dÃ©marrage-rapide)
- [Docker Compose](#-docker-compose)
- [Documentation DÃ©taillÃ©e](#-documentation-dÃ©taillÃ©e)
- [API Endpoints](#-api-endpoints)
- [ExtensibilitÃ©](#-extensibilitÃ©)
- [DÃ©ploiement](#-dÃ©ploiement)

---

## ğŸ› ï¸ Stack Technique

### Backend
- **Python 3.12.6** - Runtime
- **Flask** - Framework web avec support SSE (Server-Sent Events)
- **OpenAI GPT-4o** - Analyse Due Diligence avec tempÃ©rature 0.3
- **Firecrawl** - Scraping web robuste (15 scrapes/profil)
- **Serper** - API de recherche Google
- **Pappers API** - DonnÃ©es lÃ©gales et financiÃ¨res franÃ§aises
- **DVF (open data)** - Transactions immobiliÃ¨res franÃ§aises
- **HATVP (open data)** - Personnes politiquement exposÃ©es (PPE)
- **SQLite** - Cache avec expiration configurable (7 jours par dÃ©faut)
- **Pydantic** - Validation des schÃ©mas v3 (18 sections)
- **Jinja2** - Templates pour les prompts avec exemples inline

### Frontend
- **React 18** - Interface utilisateur moderne avec 6 onglets
- **Vite** - Build tool ultra-rapide
- **Tailwind CSS** - Styling responsive et professionnel
- **Server-Sent Events** - Progression temps rÃ©el (~2-3min)
- **Layout dynamique** - CÃ´te Ã  cÃ´te sur desktop, empilÃ© sur mobile

### DevOps
- **Docker** - Containerisation
- **Docker Compose** - Orchestration multi-services
- **Gunicorn + Gevent** - Serveur WSGI performant
- **Nginx** - Serveur web pour le frontend

---

## âœ¨ FonctionnalitÃ©s

### Backend (v3)
- âœ… **Due Diligence enrichie** - 18 sections vs 11 en v2 (psychologie, finances, rÃ©seau, analyse juridique)
- âœ… **Sources officielles FR** - Pappers (lÃ©gal/financier), DVF (immobilier), HATVP (PPE)
- âœ… **SSE streaming** - Progression temps rÃ©el avec 6 Ã©tapes (~2-3min)
- âœ… **Double sÃ©curitÃ©** - Prompt renforcÃ© + validation Python post-LLM (anti-hallucination)
- âœ… **Scoring de risque** - CrÃ©dibilitÃ©, rÃ©putation, influence, fiabilitÃ© (/100) + niveau de risque
- âœ… **Red flags** - DÃ©tection automatique avec sÃ©vÃ©ritÃ© (Critique/ModÃ©rÃ©/Mineur)
- âœ… **TraÃ§abilitÃ©** - Sources obligatoires pour chaque donnÃ©e financiÃ¨re
- âœ… **Architecture modulaire** - SystÃ¨me de sources extensible
- âœ… **Cache SQLite intelligent** - TTL configurable + force refresh
- âœ… **Analyse en franÃ§ais** - Tous les rÃ©sultats structurÃ©s en franÃ§ais

### Frontend (v3)
- âœ… **6 onglets organisÃ©s** - Vue d'ensemble, ExpÃ©rience, Financier, MÃ©dias, RÃ©seau, Analyse
- âœ… **Progression temps rÃ©el** - Barre de progression SSE avec 6 Ã©tapes visuelles
- âœ… **Scores visuels** - Affichage des 4 scores (/100) + badge niveau de risque
- âœ… **Red flags avec badges** - Alertes colorÃ©es par sÃ©vÃ©ritÃ©
- âœ… **Timeline chronologique** - Visualisation de la carriÃ¨re avec dots et lignes
- âœ… **TraÃ§abilitÃ© sources** - Affichage des sources sous chaque donnÃ©e financiÃ¨re
- âœ… **Layout dynamique** - Formulaire centrÃ© par dÃ©faut, se dÃ©place Ã  gauche au chargement
- âœ… **Responsive design** - Mobile-first avec breakpoints Tailwind

---

## ğŸ—ï¸ Architecture

```
lumiron-scraper/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Factory Flask + CORS
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ database.py          # Setup SQLite
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ person_profile.py    # SchÃ©mas Pydantic v3 (18 sections)
â”‚   â”‚   â”‚   â””â”€â”€ person_profile_v3.py # ModÃ¨les enrichis
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â””â”€â”€ api_routes.py        # Endpoints REST + SSE streaming
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ cache_service.py     # Gestion cache SQLite
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py       # GPT-4o + validation post-LLM
â”‚   â”‚   â”‚   â”œâ”€â”€ profile_service.py   # Orchestration
â”‚   â”‚   â”‚   â””â”€â”€ scraper_service.py   # Pipeline scraping
â”‚   â”‚   â”œâ”€â”€ sources/                 # ğŸ”Œ Architecture modulaire
â”‚   â”‚   â”‚   â”œâ”€â”€ base_source.py       # Classe abstraite
â”‚   â”‚   â”‚   â”œâ”€â”€ serper_search_source.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pappers_source.py    # DonnÃ©es lÃ©gales FR
â”‚   â”‚   â”‚   â”œâ”€â”€ dvf_source.py        # Immobilier FR
â”‚   â”‚   â”‚   â””â”€â”€ hatvp_source.py      # PPE FR
â”‚   â”‚   â”œâ”€â”€ templates/prompts/       # Templates Jinja2 + exemples inline
â”‚   â”‚   â”‚   â””â”€â”€ due_diligence_analysis.txt  # Prompt v3
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ url_validator.py
â”‚   â”œâ”€â”€ data/                        # SQLite DB (auto-crÃ©Ã©)
â”‚   â”‚   â””â”€â”€ lumironscraper.db
â”‚   â”œâ”€â”€ main.py                      # Point d'entrÃ©e
â”‚   â”œâ”€â”€ wsgi.py                      # WSGI pour Gunicorn
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md                    # ğŸ“˜ Documentation backend
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchForm.jsx       # Formulaire + force refresh
â”‚   â”‚   â”‚   â””â”€â”€ ProfileResults.jsx   # 6 onglets + 18 sections v3
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js               # Client SSE + fetch
â”‚   â”‚   â”œâ”€â”€ App.jsx                  # Layout dynamique + progression SSE
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css                # Animations custom
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md                    # ğŸ“˜ Documentation frontend
â”‚
â”œâ”€â”€ docker-compose.yml               # ğŸ³ Orchestration
â””â”€â”€ README.md                        # ğŸ“˜ Ce fichier
```

---

## ğŸš€ DÃ©marrage Rapide

### Option 1 : Docker Compose (RecommandÃ©)

```bash
# 1. Configuration
cp backend/.env.example backend/.env
nano backend/.env  # Ajouter vos clÃ©s API

# 2. Lancer l'application
docker-compose up -d

# 3. AccÃ©der
# Frontend: http://localhost:5101
# Backend:  http://localhost:5100
# Health:   http://localhost:5100/api/v1/health
```

---

### Option 2 : DÃ©veloppement Local

#### Backend

```bash
cd backend

# Environnement virtuel
python3.12 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# DÃ©pendances
pip install -r requirements.txt

# Configuration
cp .env.example .env
nano .env  # Ajouter OPENAI_API_KEY, FIRECRAWL_API_KEY, SERPER_API_KEY, PAPPERS_API_KEY

# Lancer
python main.py
# â†’ http://localhost:5100
```

#### Frontend

```bash
cd frontend

# DÃ©pendances
npm install

# Configuration (optionnel)
cp .env.example .env

# Lancer
npm run dev
# â†’ http://localhost:5173
```

**Documentation dÃ©taillÃ©e:**
- Backend : [backend/README.md](./backend/README.md)
- Frontend : [frontend/README.md](./frontend/README.md)

---

## ğŸ³ Docker Compose

### Configuration Frontend

Le frontend nÃ©cessite l'URL de l'API au **moment du build** (pas au runtime) :

```bash
# 1. Configurer l'URL de l'API
cd frontend
cp .env.example .env
nano .env  # Ã‰diter VITE_API_URL

# 2. Lancer avec Docker Compose
cd ..
docker-compose up -d --build
```

Le `docker-compose.yml` passe automatiquement `VITE_API_URL` depuis `frontend/.env` comme build argument.

### DÃ©marrage

```bash
docker-compose up -d
```

### Logs

```bash
docker-compose logs -f
docker-compose logs -f backend
docker-compose logs -f frontend
```

### Rebuild aprÃ¨s changement d'URL

```bash
# Si vous modifiez VITE_API_URL dans frontend/.env
docker-compose up -d --build frontend
```

### Persistance des donnÃ©es

La base de donnÃ©es SQLite est automatiquement persistÃ©e via un volume Docker :

```yaml
volumes:
  - ./backend/data:/app/data
```

**Emplacement local:** `./backend/data/lumironscraper.db`

### Healthcheck

Le backend inclut un healthcheck Python natif (pas besoin de curl) :

```yaml
healthcheck:
  test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5100/api/v1/health').read()"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```

Le frontend attend que le backend soit "healthy" avant de dÃ©marrer.

---

## ğŸ“š Documentation DÃ©taillÃ©e

| Document | Description |
|----------|-------------|
| [backend/README.md](./backend/README.md) | Installation backend, configuration, architecture sources, deployment |
| [frontend/README.md](./frontend/README.md) | Setup frontend, composants React, responsive design, deployment |

---

## ğŸ“¡ API Endpoints

### `GET /api/v1/health`

VÃ©rifie l'Ã©tat du serveur.

**RÃ©ponse:**
```json
{
  "status": "healthy",
  "message": "LumironScraper API is running"
}
```

---

### `POST /api/v1/search-stream` (RecommandÃ©)

Recherche avec streaming SSE - Progression temps rÃ©el.

**Body:**
```json
{
  "first_name": "Anthony",
  "last_name": "Tartour",
  "company": "Lumiron",
  "force_refresh": false
}
```

**RÃ©ponse (Server-Sent Events):**
```
data: {"type":"progress","step":"cache","percent":5,"message":"VÃ©rification du cache..."}
data: {"type":"progress","step":"pappers","percent":15,"message":"RÃ©cupÃ©ration donnÃ©es Pappers..."}
data: {"type":"progress","step":"scraping","percent":50,"message":"Scraping des pages (15 scrapes, ~2min)..."}
data: {"type":"progress","step":"analysis","percent":85,"message":"Analyse GPT-4o..."}
data: {"type":"complete","data":{...profil v3...}}
```

---

### `POST /api/v1/search`

Recherche classique (sans streaming).

**Body:**
```json
{
  "first_name": "Anthony",
  "last_name": "Tartour",
  "company": "Lumiron",
  "force_refresh": false
}
```

**RÃ©ponse (v3 - 18 sections):**
```json
{
  "success": true,
  "cached": false,
  "data": {
    "full_name": "Anthony Tartour",
    "current_position": "Co-Founder",
    "company": "Lumiron",
    "credibility_score": 75,
    "reputation_score": 80,
    "influence_score": 65,
    "reliability_score": 70,
    "risk_level": "Moyen",
    "professional_experience": [...],
    "business_ecosystem": {...},
    "financial_intelligence": {...},
    "psychology_and_approach": {...},
    "media_presence": {...},
    "red_flags": [...],
    "career_timeline": [...],
    // + 12 autres sections
  }
}
```

---

### `GET /api/v1/cache/stats`

Statistiques du cache.

**RÃ©ponse:**
```json
{
  "total_entries": 42,
  "cache_size": "2.5 MB",
  "oldest_entry": "2025-11-27T10:00:00",
  "newest_entry": "2025-12-04T15:30:00"
}
```

---

### `POST /api/v1/cache/clear-expired`

Nettoie les entrÃ©es expirÃ©es du cache.

**RÃ©ponse:**
```json
{
  "success": true,
  "deleted_entries": 5
}
```

---

## ğŸ”Œ ExtensibilitÃ©

### Architecture Modulaire des Sources

LumironScraper utilise un systÃ¨me de **sources modulaires** pour le scraping. <br/>
Chaque source hÃ©rite de `BaseSource` et est automatiquement chargÃ©e.

#### Ajouter une nouvelle source

**1. CrÃ©er le fichier** `backend/app/sources/my_source.py`

```python
from typing import List
from app.sources.base_source import BaseSource

class MySource(BaseSource):
    @classmethod
    def get_name(cls) -> str:
        return "my_source"

    @classmethod
    def get_description(cls) -> str:
        return "Ma source personnalisÃ©e"

    def get_urls(self, first_name: str, last_name: str, company: str) -> List[str]:
        # Logique pour gÃ©nÃ©rer les URLs Ã  scraper
        return [
            f"https://example.com/search?q={first_name}+{last_name}",
            # ...
        ]
```

**2. Enregistrer** dans `backend/app/sources/__init__.py`

```python
from app.sources.my_source import MySource

AVAILABLE_SOURCES = [
    SerperSearchSource,
    CompanyWebsiteSource,
    MySource,  # â† Ajouter ici
]
```

**3. RedÃ©marrer** le backend

```bash
docker-compose restart backend
# ou
python main.py
```

**C'est tout !** La source sera automatiquement utilisÃ©e dans le pipeline de scraping.

---

### Modifier le prompt LLM

Les prompts sont des **templates Jinja2** Ã©ditables sans toucher au code :

```bash
nano backend/app/templates/prompts/due_diligence_analysis.txt
```

Le prompt v3 inclut des **exemples inline** directement dans la structure JSON pour guider GPT-4o.

RedÃ©marrer pour appliquer les changements.

---

### Changer de LLM

Modifier `backend/app/services/llm_service.py` :

```python
# Exemple avec Anthropic Claude
from anthropic import Anthropic

client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
```

---

## ğŸš¢ DÃ©ploiement

### Backend

#### Heroku

```bash
heroku create lumironscraper-backend
heroku config:set OPENAI_API_KEY=sk-...
heroku config:set FIRECRAWL_API_KEY=fc-...
heroku config:set SERPER_API_KEY=...
heroku config:set PAPPERS_API_KEY=...
git push heroku main
```

#### Railway

1. Connecter le repo GitHub
2. Ajouter les variables d'environnement
3. Deploy automatique via `Procfile`

#### VPS

```bash
# Installer Python 3.12
sudo apt install python3.12 python3.12-venv

# Clone et setup
git clone https://github.com/Crisxzu/lumiron-scraper.git
cd LumironScraper/backend
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Configuration
nano .env

# Lancer avec Gunicorn
gunicorn --bind 0.0.0.0:5100 --workers 4 "app:create_app()"
```

---

### Frontend

#### Netlify

```bash
# Build command
npm run build

# Publish directory
dist

# Environment variables
VITE_API_URL=https://your-api.com/api/v1
```

#### Vercel

```bash
vercel --prod
```

Configuration automatique via `vite.config.js`.

---

## ğŸ”’ SÃ©curitÃ©

- âœ… Toutes les clÃ©s API dans des variables d'environnement
- âœ… CORS configurable via `.env`
- âœ… Validation des entrÃ©es avec Pydantic
- âœ… Pas de stockage de donnÃ©es sensibles
- âœ… HTTPS recommandÃ© en production
- âœ… Healthcheck sans exposition de donnÃ©es sensibles

---

## ğŸ“Š Configuration

### Variables d'environnement Backend

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `OPENAI_API_KEY` | - | ClÃ© API OpenAI (obligatoire) |
| `FIRECRAWL_API_KEY` | - | ClÃ© API Firecrawl (obligatoire) |
| `SERPER_API_KEY` | - | ClÃ© API Serper (obligatoire) |
| `PAPPERS_API_KEY` | - | ClÃ© API Pappers (obligatoire) |
| `PAPPERS_MODE` | `standard` | Mode Pappers (`standard` ou `complete`) |
| `PORT` | `5100` | Port du backend |
| `FLASK_DEBUG` | `0` | Mode debug (0=prod, 1=dev) |
| `OPENAI_MODEL` | `gpt-4o` | ModÃ¨le OpenAI |
| `MAX_TOTAL_SCRAPES` | `15` | Nombre max de scrapes (v3) |
| `DATABASE_PATH` | `data/lumironscraper.db` | Chemin de la DB SQLite |
| `CACHE_TTL_SECONDS` | `604800` | TTL du cache (7 jours) |
| `CORS_ORIGINS` | `http://localhost:5101,...` | Origins CORS autorisÃ©es |

### Variables d'environnement Frontend

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `VITE_API_URL` | `http://localhost:5100/api/v1` | URL de l'API backend |

---

## ğŸ¯ Utilisation

1. **Ouvrir l'interface** : http://localhost:5101
2. **Remplir le formulaire** :
   - PrÃ©nom (ex: Anthony)
   - Nom (ex: Tartour)
   - Entreprise (ex: Lumiron)
   - â˜‘ï¸ Force refresh (optionnel, pour ignorer le cache)
3. **Cliquer sur "Rechercher"**
4. **Suivre la progression** (6 Ã©tapes, ~2-3min) :
   - VÃ©rification cache â†’ Pappers â†’ DVF â†’ HATVP â†’ Scraping â†’ Analyse GPT-4o
5. **Consulter le profil v3** (6 onglets) :
   - **Vue d'ensemble** : Scores, niveau de risque, rÃ©sumÃ©, red flags
   - **ExpÃ©rience** : Parcours professionnel, timeline chronologique
   - **Financier** : Intelligence financiÃ¨re avec sources, Ã©cosystÃ¨me d'affaires, patrimoine immobilier, PPE
   - **MÃ©dias & RÃ©putation** : PrÃ©sence mÃ©diatique avec sentiment, publications
   - **RÃ©seau & Influence** : Connexions, indicateurs d'influence
   - **Analyse** : Psychologie avec traits justifiÃ©s, ice breakers, cohÃ©rence

---

## ğŸ› Troubleshooting

### Backend ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker-compose logs backend

# VÃ©rifier les variables d'environnement
docker-compose exec backend env | grep API_KEY
```

### Frontend ne peut pas contacter le backend

```bash
# VÃ©rifier que le backend est accessible
curl http://localhost:5100/api/v1/health

# VÃ©rifier les CORS
docker-compose logs backend | grep CORS
```

### Base de donnÃ©es verrouillÃ©e

```bash
docker-compose down
docker-compose up -d
```
