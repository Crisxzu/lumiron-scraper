# üîç LumironScraper

**Intelligence de profils professionnels pour accompagner vos d√©marches commerciales**

Solution full-stack de scraping et d'analyse de profils professionnels. √Ä partir d'un pr√©nom, nom et entreprise, le syst√®me collecte des informations publiques sur le web et utilise l'IA pour g√©n√©rer un profil structur√© en fran√ßais.

---

## üìã Table des mati√®res

- [Stack Technique](#-stack-technique)
- [Fonctionnalit√©s](#-fonctionnalit√©s)
- [Architecture](#-architecture)
- [D√©marrage Rapide](#-d√©marrage-rapide)
- [Docker Compose](#-docker-compose)
- [Documentation D√©taill√©e](#-documentation-d√©taill√©e)
- [API Endpoints](#-api-endpoints)
- [Extensibilit√©](#-extensibilit√©)
- [D√©ploiement](#-d√©ploiement)

---

## üõ†Ô∏è Stack Technique

### Backend
- **Python 3.12.6** - Runtime
- **Flask** - Framework web l√©ger et performant
- **OpenAI GPT-4o** - Analyse et structuration des donn√©es en fran√ßais
- **Firecrawl** - Scraping web robuste
- **Serper** - API de recherche Google
- **SQLite** - Cache avec expiration configurable (7 jours par d√©faut)
- **Pydantic** - Validation des sch√©mas de donn√©es
- **Jinja2** - Templates pour les prompts LLM

### Frontend
- **React 18** - Interface utilisateur moderne
- **Vite** - Build tool ultra-rapide
- **Tailwind CSS** - Styling responsive et professionnel
- **Axios** - Client HTTP
- **Layout dynamique** - C√¥te √† c√¥te sur desktop, empil√© sur mobile

### DevOps
- **Docker** - Containerisation
- **Docker Compose** - Orchestration multi-services
- **Gunicorn + Gevent** - Serveur WSGI performant
- **Nginx** - Serveur web pour le frontend

---

## ‚ú® Fonctionnalit√©s

### Backend
- ‚úÖ **Architecture modulaire** - Syst√®me de sources extensible (Serper, sites entreprises, etc.)
- ‚úÖ **Cache SQLite intelligent** - TTL configurable + statistiques
- ‚úÖ **Force refresh** - Option pour ignorer le cache
- ‚úÖ **Multi-profils LinkedIn** - Fusion automatique de tous les profils trouv√©s
- ‚úÖ **Prompts Jinja2** - Templates √©ditables sans toucher au code
- ‚úÖ **CORS configurable** - Via variable d'environnement
- ‚úÖ **Timezone-aware** - Gestion UTC pour le cache
- ‚úÖ **Health check** - Endpoint de monitoring
- ‚úÖ **Analyse LLM en fran√ßais** - Tous les r√©sultats structur√©s en fran√ßais

### Frontend
- ‚úÖ **Layout c√¥te √† c√¥te** - Formulaire gauche, r√©sultats droite (desktop)
- ‚úÖ **Animations fluides** - Slide-in depuis la droite pour les r√©sultats
- ‚úÖ **Responsive design** - Mobile-first avec breakpoints Tailwind
- ‚úÖ **Indicateur de cache** - Badge vert (cache) ou bleu (frais) avec √¢ge
- ‚úÖ **Force refresh checkbox** - Ignorer le cache facilement
- ‚úÖ **Layout dynamique** - Formulaire centr√© par d√©faut, se d√©place √† gauche au chargement

---

## üèóÔ∏è Architecture

```
lumiron-scraper/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Factory Flask + CORS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py          # Setup SQLite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ person_profile.py    # Sch√©mas Pydantic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api_routes.py        # Endpoints REST
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache_service.py     # Gestion cache SQLite
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py       # OpenAI GPT-4
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile_service.py   # Orchestration
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper_service.py   # Pipeline scraping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources/                 # üîå Architecture modulaire
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_source.py       # Classe abstraite
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ serper_search_source.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ company_website_source.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates/prompts/       # Templates Jinja2
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ profile_analysis.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ url_validator.py
‚îÇ   ‚îú‚îÄ‚îÄ data/                        # SQLite DB (auto-cr√©√©)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lumironscraper.db
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # Point d'entr√©e
‚îÇ   ‚îú‚îÄ‚îÄ wsgi.py                      # WSGI pour Gunicorn
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ README.md                    # üìò Documentation backend
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SearchForm.jsx       # Formulaire + force refresh
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ProfileResults.jsx   # Affichage profil
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.js               # Client Axios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx                  # Layout dynamique
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.css                # Animations custom
‚îÇ   ‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md                    # üìò Documentation frontend
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml               # üê≥ Orchestration
‚îî‚îÄ‚îÄ README.md                        # üìò Ce fichier
```

---

## üöÄ D√©marrage Rapide

### Option 1 : Docker Compose (Recommand√©)

```bash
# 1. Configuration
cp backend/.env.example backend/.env
nano backend/.env  # Ajouter vos cl√©s API

# 2. Lancer l'application
docker-compose up -d

# 3. Acc√©der
# Frontend: http://localhost:3000
# Backend:  http://localhost:5100
# Health:   http://localhost:5100/api/v1/health
```

---

### Option 2 : D√©veloppement Local

#### Backend

```bash
cd backend

# Environnement virtuel
python3.12 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# D√©pendances
pip install -r requirements.txt

# Configuration
cp .env.example .env
nano .env  # Ajouter OPENAI_API_KEY, FIRECRAWL_API_KEY, SERPER_API_KEY

# Lancer
python main.py
# ‚Üí http://localhost:5100
```

#### Frontend

```bash
cd frontend

# D√©pendances
npm install

# Configuration (optionnel)
cp .env.example .env

# Lancer
npm run dev
# ‚Üí http://localhost:5173
```

**Documentation d√©taill√©e:**
- Backend : [backend/README.md](./backend/README.md)
- Frontend : [frontend/README.md](./frontend/README.md)

---

## üê≥ Docker Compose

### D√©marrage

```bash
docker-compose up -d
```

### Logs

```bash
docker-compose logs -f
docker-compose logs -f backend
docker-compose logs -f frontend
```

### Persistance des donn√©es

La base de donn√©es SQLite est automatiquement persist√©e via un volume Docker :

```yaml
volumes:
  - ./backend/data:/app/data
```

**Emplacement local:** `./backend/data/lumironscraper.db`

### Healthcheck

Le backend inclut un healthcheck Python natif :

```yaml
healthcheck:
  test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5100/api/v1/health').read()"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```

Le frontend attend que le backend soit "healthy" avant de d√©marrer.

---

## üìö Documentation D√©taill√©e

| Document | Description |
|----------|-------------|
| [backend/README.md](./backend/README.md) | Installation backend, configuration, architecture sources, deployment |
| [frontend/README.md](./frontend/README.md) | Setup frontend, composants React, responsive design, deployment |

---

## üì° API Endpoints

### `GET /api/v1/health`

V√©rifie l'√©tat du serveur.

**R√©ponse:**
```json
{
  "status": "healthy",
  "message": "LumironScraper API is running"
}
```

---

### `POST /api/v1/search`

Recherche et analyse un profil professionnel.

**Body:**
```json
{
  "first_name": "Satya",
  "last_name": "Nadella",
  "company": "Microsoft",
  "force_refresh": false  // Optionnel
}
```

**R√©ponse:**
```json
{
  "success": true,
  "cached": true,
  "cache_age_seconds": 3600,
  "cache_created_at": "2025-12-04T10:00:00",
  "data": {
    "full_name": "Satya Nadella",
    "current_position": "Directeur G√©n√©ral",
    "company": "Microsoft",
    "professional_experience": [
      {
        "position": "CEO",
        "company": "Microsoft",
        "period": "2014 - Pr√©sent",
        "description": "..."
      }
    ],
    "skills": ["Leadership", "Cloud Computing", "Transformation digitale"],
    "publications": [
      {
        "title": "Hit Refresh",
        "date": "2017",
        "description": "..."
      }
    ],
    "public_contact": {
      "email": null,
      "phone": null,
      "linkedin": "https://linkedin.com/in/satyanadella"
    },
    "summary": "Satya Nadella est le Directeur G√©n√©ral de Microsoft depuis 2014...",
    "linkedin_url": "https://linkedin.com/in/satyanadella",
    "sources": [
      "https://linkedin.com/in/satyanadella",
      "https://microsoft.com/...",
      "..."
    ]
  }
}
```

---

### `GET /api/v1/cache/stats`

Statistiques du cache.

**R√©ponse:**
```json
{
  "total_entries": 42,
  "cache_size": "2.5 MB",
  "oldest_entry": "2025-11-27T10:00:00",
  "newest_entry": "2025-12-04T15:30:00"
}
```

---

### `POST /api/v1/cache/clear-expired`

Nettoie les entr√©es expir√©es du cache.

**R√©ponse:**
```json
{
  "success": true,
  "deleted_entries": 5
}
```

---

## üîå Extensibilit√©

### Architecture Modulaire des Sources

LumironScraper utilise un syst√®me de **sources modulaires** pour le scraping. Chaque source h√©rite de `BaseSource` et est automatiquement charg√©e.

#### Ajouter une nouvelle source

**1. Cr√©er le fichier** `backend/app/sources/my_source.py`

```python
from typing import List
from app.sources.base_source import BaseSource

class MySource(BaseSource):
    @classmethod
    def get_name(cls) -> str:
        return "my_source"

    @classmethod
    def get_description(cls) -> str:
        return "Ma source personnalis√©e"

    def get_urls(self, first_name: str, last_name: str, company: str) -> List[str]:
        # Logique pour g√©n√©rer les URLs √† scraper
        return [
            f"https://example.com/search?q={first_name}+{last_name}",
            # ...
        ]
```

**2. Enregistrer** dans `backend/app/sources/__init__.py`

```python
from app.sources.my_source import MySource

AVAILABLE_SOURCES = [
    SerperSearchSource,
    CompanyWebsiteSource,
    MySource,  # ‚Üê Ajouter ici
]
```

**3. Red√©marrer** le backend

```bash
docker-compose restart backend
# ou
python main.py
```

**C'est tout !** La source sera automatiquement utilis√©e dans le pipeline de scraping.

---

### Modifier le prompt LLM

Les prompts sont des **templates Jinja2** √©ditables sans toucher au code :

```bash
nano backend/app/templates/prompts/profile_analysis.txt
```

Red√©marrer pour appliquer les changements.

---

### Changer de LLM

Modifier `backend/app/services/llm_service.py` :

```python
# Exemple avec Anthropic Claude
from anthropic import Anthropic

client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
```

---

## üö¢ D√©ploiement

### Backend

#### Heroku

```bash
heroku create lumironscraper-backend
heroku config:set OPENAI_API_KEY=sk-...
heroku config:set FIRECRAWL_API_KEY=fc-...
heroku config:set SERPER_API_KEY=...
git push heroku main
```

#### Railway

1. Connecter le repo GitHub
2. Ajouter les variables d'environnement
3. Deploy automatique via `Procfile`

#### VPS

```bash
# Installer Python 3.12
sudo apt install python3.12 python3.12-venv

# Clone et setup
git clone https://github.com/Crisxzu/lumiron-scraper.git
cd LumironScraper/backend
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Configuration
nano .env

# Lancer avec Gunicorn
gunicorn --bind 0.0.0.0:5100 --workers 4 "app:create_app()"
```

---

### Frontend

#### Netlify

```bash
# Build command
npm run build

# Publish directory
dist

# Environment variables
VITE_API_URL=https://your-api.com/api/v1
```

#### Vercel

```bash
vercel --prod
```

Configuration automatique via `vite.config.js`.

---

## üîí S√©curit√©

- ‚úÖ Toutes les cl√©s API dans des variables d'environnement
- ‚úÖ CORS configurable via `.env`
- ‚úÖ Validation des entr√©es avec Pydantic
- ‚úÖ Pas de stockage de donn√©es sensibles
- ‚úÖ HTTPS recommand√© en production
- ‚úÖ Healthcheck sans exposition de donn√©es sensibles

---

## üìä Configuration

### Variables d'environnement Backend

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `OPENAI_API_KEY` | - | Cl√© API OpenAI (obligatoire) |
| `FIRECRAWL_API_KEY` | - | Cl√© API Firecrawl (obligatoire) |
| `SERPER_API_KEY` | - | Cl√© API Serper (obligatoire) |
| `PORT` | `5100` | Port du backend |
| `FLASK_DEBUG` | `0` | Mode debug (0=prod, 1=dev) |
| `OPENAI_MODEL` | `gpt-4o` | Mod√®le OpenAI |
| `MAX_TOTAL_SCRAPES` | `3` | Nombre max de scrapes |
| `DATABASE_PATH` | `data/lumironscraper.db` | Chemin de la DB SQLite |
| `CACHE_TTL_SECONDS` | `604800` | TTL du cache (7 jours) |
| `CORS_ORIGINS` | `http://localhost:3000,...` | Origins CORS autoris√©es |

### Variables d'environnement Frontend

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `VITE_API_URL` | `http://localhost:5100/api/v1` | URL de l'API backend |

---

## üéØ Utilisation

1. **Ouvrir l'interface** : http://localhost:3000
2. **Remplir le formulaire** :
   - Pr√©nom (ex: Satya)
   - Nom (ex: Nadella)
   - Entreprise (ex: Microsoft)
   - ‚òëÔ∏è Force refresh (optionnel, pour ignorer le cache)
3. **Cliquer sur "Rechercher"**
4. **Consulter le profil** structur√© :
   - Nom complet + poste actuel
   - R√©sum√© professionnel
   - Exp√©rience d√©taill√©e
   - Comp√©tences
   - Publications
   - Contact public
   - Sources utilis√©es

**Badge de cache :**
- üü¢ Vert : Donn√©es du cache (avec √¢ge en minutes)
- üîµ Bleu : Donn√©es fra√Æches (nouvellement scrap√©es)

---

## üêõ Troubleshooting

### Backend ne d√©marre pas

```bash
# V√©rifier les logs
docker-compose logs backend

# V√©rifier les variables d'environnement
docker-compose exec backend env | grep API_KEY
```

### Frontend ne peut pas contacter le backend

```bash
# V√©rifier que le backend est accessible
curl http://localhost:5100/api/v1/health

# V√©rifier les CORS
docker-compose logs backend | grep CORS
```

### Base de donn√©es verrouill√©e

```bash
docker-compose down
docker-compose up -d
```
