# ğŸ” LumironScraper

**Intelligence de profils professionnels pour accompagner vos dÃ©marches commerciales**

Solution full-stack de scraping et d'analyse de profils professionnels. <br/>
Ã€ partir d'un prÃ©nom, nom et entreprise, le systÃ¨me collecte des informations publiques sur le web et utilise l'IA pour gÃ©nÃ©rer un profil structurÃ© en franÃ§ais.

---

## ğŸ“‹ Table des matiÃ¨res

- [Stack Technique](#-stack-technique)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [DÃ©marrage Rapide](#-dÃ©marrage-rapide)
- [Docker Compose](#-docker-compose)
- [Documentation DÃ©taillÃ©e](#-documentation-dÃ©taillÃ©e)
- [API Endpoints](#-api-endpoints)
- [ExtensibilitÃ©](#-extensibilitÃ©)
- [DÃ©ploiement](#-dÃ©ploiement)

---

## ğŸ› ï¸ Stack Technique

### Backend
- **Python 3.12.6** - Runtime
- **Flask** - Framework web lÃ©ger et performant
- **OpenAI GPT-4o** - Analyse et structuration des donnÃ©es en franÃ§ais
- **Firecrawl** - Scraping web robuste
- **Serper** - API de recherche Google
- **SQLite** - Cache avec expiration configurable (7 jours par dÃ©faut)
- **Pydantic** - Validation des schÃ©mas de donnÃ©es
- **Jinja2** - Templates pour les prompts LLM

### Frontend
- **React 18** - Interface utilisateur moderne
- **Vite** - Build tool ultra-rapide
- **Tailwind CSS** - Styling responsive et professionnel
- **Axios** - Client HTTP
- **Layout dynamique** - CÃ´te Ã  cÃ´te sur desktop, empilÃ© sur mobile

### DevOps
- **Docker** - Containerisation
- **Docker Compose** - Orchestration multi-services
- **Gunicorn + Gevent** - Serveur WSGI performant
- **Nginx** - Serveur web pour le frontend

---

## âœ¨ FonctionnalitÃ©s

### Backend
- âœ… **Architecture modulaire** - SystÃ¨me de sources extensible (Serper, sites entreprises, etc.)
- âœ… **Cache SQLite intelligent** - TTL configurable + statistiques
- âœ… **Force refresh** - Option pour ignorer le cache
- âœ… **Multi-profils LinkedIn** - Fusion automatique de tous les profils trouvÃ©s
- âœ… **Prompts Jinja2** - Templates Ã©ditables sans toucher au code
- âœ… **CORS configurable** - Via variable d'environnement
- âœ… **Timezone-aware** - Gestion UTC pour le cache
- âœ… **Health check** - Endpoint de monitoring
- âœ… **Analyse LLM en franÃ§ais** - Tous les rÃ©sultats structurÃ©s en franÃ§ais

### Frontend
- âœ… **Layout cÃ´te Ã  cÃ´te** - Formulaire gauche, rÃ©sultats droite (desktop)
- âœ… **Animations fluides** - Slide-in depuis la droite pour les rÃ©sultats
- âœ… **Responsive design** - Mobile-first avec breakpoints Tailwind
- âœ… **Indicateur de cache** - Badge vert (cache) ou bleu (frais) avec Ã¢ge
- âœ… **Force refresh checkbox** - Ignorer le cache facilement
- âœ… **Layout dynamique** - Formulaire centrÃ© par dÃ©faut, se dÃ©place Ã  gauche au chargement

---

## ğŸ—ï¸ Architecture

```
lumiron-scraper/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py              # Factory Flask + CORS
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â””â”€â”€ database.py          # Setup SQLite
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ person_profile.py    # SchÃ©mas Pydantic
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â””â”€â”€ api_routes.py        # Endpoints REST
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ cache_service.py     # Gestion cache SQLite
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py       # OpenAI GPT-4
â”‚   â”‚   â”‚   â”œâ”€â”€ profile_service.py   # Orchestration
â”‚   â”‚   â”‚   â””â”€â”€ scraper_service.py   # Pipeline scraping
â”‚   â”‚   â”œâ”€â”€ sources/                 # ğŸ”Œ Architecture modulaire
â”‚   â”‚   â”‚   â”œâ”€â”€ base_source.py       # Classe abstraite
â”‚   â”‚   â”‚   â”œâ”€â”€ serper_search_source.py
â”‚   â”‚   â”‚   â””â”€â”€ company_website_source.py
â”‚   â”‚   â”œâ”€â”€ templates/prompts/       # Templates Jinja2
â”‚   â”‚   â”‚   â””â”€â”€ profile_analysis.txt
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ url_validator.py
â”‚   â”œâ”€â”€ data/                        # SQLite DB (auto-crÃ©Ã©)
â”‚   â”‚   â””â”€â”€ lumironscraper.db
â”‚   â”œâ”€â”€ main.py                      # Point d'entrÃ©e
â”‚   â”œâ”€â”€ wsgi.py                      # WSGI pour Gunicorn
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md                    # ğŸ“˜ Documentation backend
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchForm.jsx       # Formulaire + force refresh
â”‚   â”‚   â”‚   â””â”€â”€ ProfileResults.jsx   # Affichage profil
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js               # Client Axios
â”‚   â”‚   â”œâ”€â”€ App.jsx                  # Layout dynamique
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css                # Animations custom
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md                    # ğŸ“˜ Documentation frontend
â”‚
â”œâ”€â”€ docker-compose.yml               # ğŸ³ Orchestration
â””â”€â”€ README.md                        # ğŸ“˜ Ce fichier
```

---

## ğŸš€ DÃ©marrage Rapide

### Option 1 : Docker Compose (RecommandÃ©)

```bash
# 1. Configuration
cp backend/.env.example backend/.env
nano backend/.env  # Ajouter vos clÃ©s API

# 2. Lancer l'application
docker-compose up -d

# 3. AccÃ©der
# Frontend: http://localhost:5101
# Backend:  http://localhost:5100
# Health:   http://localhost:5100/api/v1/health
```

---

### Option 2 : DÃ©veloppement Local

#### Backend

```bash
cd backend

# Environnement virtuel
python3.12 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# DÃ©pendances
pip install -r requirements.txt

# Configuration
cp .env.example .env
nano .env  # Ajouter OPENAI_API_KEY, FIRECRAWL_API_KEY, SERPER_API_KEY

# Lancer
python main.py
# â†’ http://localhost:5100
```

#### Frontend

```bash
cd frontend

# DÃ©pendances
npm install

# Configuration (optionnel)
cp .env.example .env

# Lancer
npm run dev
# â†’ http://localhost:5173
```

**Documentation dÃ©taillÃ©e:**
- Backend : [backend/README.md](./backend/README.md)
- Frontend : [frontend/README.md](./frontend/README.md)

---

## ğŸ³ Docker Compose

### Configuration Frontend

Le frontend nÃ©cessite l'URL de l'API au **moment du build** (pas au runtime) :

```bash
# 1. Configurer l'URL de l'API
cd frontend
cp .env.example .env
nano .env  # Ã‰diter VITE_API_URL

# 2. Lancer avec Docker Compose
cd ..
docker-compose up -d --build
```

Le `docker-compose.yml` passe automatiquement `VITE_API_URL` depuis `frontend/.env` comme build argument.

### DÃ©marrage

```bash
docker-compose up -d
```

### Logs

```bash
docker-compose logs -f
docker-compose logs -f backend
docker-compose logs -f frontend
```

### Rebuild aprÃ¨s changement d'URL

```bash
# Si vous modifiez VITE_API_URL dans frontend/.env
docker-compose up -d --build frontend
```

### Persistance des donnÃ©es

La base de donnÃ©es SQLite est automatiquement persistÃ©e via un volume Docker :

```yaml
volumes:
  - ./backend/data:/app/data
```

**Emplacement local:** `./backend/data/lumironscraper.db`

### Healthcheck

Le backend inclut un healthcheck Python natif (pas besoin de curl) :

```yaml
healthcheck:
  test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5100/api/v1/health').read()"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```

Le frontend attend que le backend soit "healthy" avant de dÃ©marrer.

---

## ğŸ“š Documentation DÃ©taillÃ©e

| Document | Description |
|----------|-------------|
| [backend/README.md](./backend/README.md) | Installation backend, configuration, architecture sources, deployment |
| [frontend/README.md](./frontend/README.md) | Setup frontend, composants React, responsive design, deployment |

---

## ğŸ“¡ API Endpoints

### `GET /api/v1/health`

VÃ©rifie l'Ã©tat du serveur.

**RÃ©ponse:**
```json
{
  "status": "healthy",
  "message": "LumironScraper API is running"
}
```

---

### `POST /api/v1/search`

Recherche et analyse un profil professionnel.

**Body:**
```json
{
  "first_name": "Satya",
  "last_name": "Nadella",
  "company": "Microsoft",
  "force_refresh": false  // Optionnel
}
```

**RÃ©ponse:**
```json
{
  "success": true,
  "cached": true,
  "cache_age_seconds": 3600,
  "cache_created_at": "2025-12-04T10:00:00",
  "data": {
    "full_name": "Satya Nadella",
    "current_position": "Directeur GÃ©nÃ©ral",
    "company": "Microsoft",
    "professional_experience": [
      {
        "position": "CEO",
        "company": "Microsoft",
        "period": "2014 - PrÃ©sent",
        "description": "..."
      }
    ],
    "skills": ["Leadership", "Cloud Computing", "Transformation digitale"],
    "publications": [
      {
        "title": "Hit Refresh",
        "date": "2017",
        "description": "..."
      }
    ],
    "public_contact": {
      "email": null,
      "phone": null,
      "linkedin": "https://linkedin.com/in/satyanadella"
    },
    "summary": "Satya Nadella est le Directeur GÃ©nÃ©ral de Microsoft depuis 2014...",
    "linkedin_url": "https://linkedin.com/in/satyanadella",
    "sources": [
      "https://linkedin.com/in/satyanadella",
      "https://microsoft.com/...",
      "..."
    ]
  }
}
```

---

### `GET /api/v1/cache/stats`

Statistiques du cache.

**RÃ©ponse:**
```json
{
  "total_entries": 42,
  "cache_size": "2.5 MB",
  "oldest_entry": "2025-11-27T10:00:00",
  "newest_entry": "2025-12-04T15:30:00"
}
```

---

### `POST /api/v1/cache/clear-expired`

Nettoie les entrÃ©es expirÃ©es du cache.

**RÃ©ponse:**
```json
{
  "success": true,
  "deleted_entries": 5
}
```

---

## ğŸ”Œ ExtensibilitÃ©

### Architecture Modulaire des Sources

LumironScraper utilise un systÃ¨me de **sources modulaires** pour le scraping. <br/>
Chaque source hÃ©rite de `BaseSource` et est automatiquement chargÃ©e.

#### Ajouter une nouvelle source

**1. CrÃ©er le fichier** `backend/app/sources/my_source.py`

```python
from typing import List
from app.sources.base_source import BaseSource

class MySource(BaseSource):
    @classmethod
    def get_name(cls) -> str:
        return "my_source"

    @classmethod
    def get_description(cls) -> str:
        return "Ma source personnalisÃ©e"

    def get_urls(self, first_name: str, last_name: str, company: str) -> List[str]:
        # Logique pour gÃ©nÃ©rer les URLs Ã  scraper
        return [
            f"https://example.com/search?q={first_name}+{last_name}",
            # ...
        ]
```

**2. Enregistrer** dans `backend/app/sources/__init__.py`

```python
from app.sources.my_source import MySource

AVAILABLE_SOURCES = [
    SerperSearchSource,
    CompanyWebsiteSource,
    MySource,  # â† Ajouter ici
]
```

**3. RedÃ©marrer** le backend

```bash
docker-compose restart backend
# ou
python main.py
```

**C'est tout !** La source sera automatiquement utilisÃ©e dans le pipeline de scraping.

---

### Modifier le prompt LLM

Les prompts sont des **templates Jinja2** Ã©ditables sans toucher au code :

```bash
nano backend/app/templates/prompts/profile_analysis.txt
```

RedÃ©marrer pour appliquer les changements.

---

### Changer de LLM

Modifier `backend/app/services/llm_service.py` :

```python
# Exemple avec Anthropic Claude
from anthropic import Anthropic

client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
```

---

## ğŸš¢ DÃ©ploiement

### Backend

#### Heroku

```bash
heroku create lumironscraper-backend
heroku config:set OPENAI_API_KEY=sk-...
heroku config:set FIRECRAWL_API_KEY=fc-...
heroku config:set SERPER_API_KEY=...
git push heroku main
```

#### Railway

1. Connecter le repo GitHub
2. Ajouter les variables d'environnement
3. Deploy automatique via `Procfile`

#### VPS

```bash
# Installer Python 3.12
sudo apt install python3.12 python3.12-venv

# Clone et setup
git clone https://github.com/Crisxzu/lumiron-scraper.git
cd LumironScraper/backend
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Configuration
nano .env

# Lancer avec Gunicorn
gunicorn --bind 0.0.0.0:5100 --workers 4 "app:create_app()"
```

---

### Frontend

#### Netlify

```bash
# Build command
npm run build

# Publish directory
dist

# Environment variables
VITE_API_URL=https://your-api.com/api/v1
```

#### Vercel

```bash
vercel --prod
```

Configuration automatique via `vite.config.js`.

---

## ğŸ”’ SÃ©curitÃ©

- âœ… Toutes les clÃ©s API dans des variables d'environnement
- âœ… CORS configurable via `.env`
- âœ… Validation des entrÃ©es avec Pydantic
- âœ… Pas de stockage de donnÃ©es sensibles
- âœ… HTTPS recommandÃ© en production
- âœ… Healthcheck sans exposition de donnÃ©es sensibles

---

## ğŸ“Š Configuration

### Variables d'environnement Backend

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `OPENAI_API_KEY` | - | ClÃ© API OpenAI (obligatoire) |
| `FIRECRAWL_API_KEY` | - | ClÃ© API Firecrawl (obligatoire) |
| `SERPER_API_KEY` | - | ClÃ© API Serper (obligatoire) |
| `PORT` | `5100` | Port du backend |
| `FLASK_DEBUG` | `0` | Mode debug (0=prod, 1=dev) |
| `OPENAI_MODEL` | `gpt-4o` | ModÃ¨le OpenAI |
| `MAX_TOTAL_SCRAPES` | `3` | Nombre max de scrapes |
| `DATABASE_PATH` | `data/lumironscraper.db` | Chemin de la DB SQLite |
| `CACHE_TTL_SECONDS` | `604800` | TTL du cache (7 jours) |
| `CORS_ORIGINS` | `http://localhost:5101,...` | Origins CORS autorisÃ©es |

### Variables d'environnement Frontend

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `VITE_API_URL` | `http://localhost:5100/api/v1` | URL de l'API backend |

---

## ğŸ¯ Utilisation

1. **Ouvrir l'interface** : http://localhost:5101
2. **Remplir le formulaire** :
   - PrÃ©nom (ex: Satya)
   - Nom (ex: Nadella)
   - Entreprise (ex: Microsoft)
   - â˜‘ï¸ Force refresh (optionnel, pour ignorer le cache)
3. **Cliquer sur "Rechercher"**
4. **Consulter le profil** structurÃ© :
   - Nom complet + poste actuel
   - RÃ©sumÃ© professionnel
   - ExpÃ©rience dÃ©taillÃ©e
   - CompÃ©tences
   - Publications
   - Contact public
   - Sources utilisÃ©es

**Badge de cache :**
- ğŸŸ¢ Vert : DonnÃ©es du cache (avec Ã¢ge en minutes)
- ğŸ”µ Bleu : DonnÃ©es fraÃ®ches (nouvellement scrapÃ©es)

---

## ğŸ› Troubleshooting

### Backend ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker-compose logs backend

# VÃ©rifier les variables d'environnement
docker-compose exec backend env | grep API_KEY
```

### Frontend ne peut pas contacter le backend

```bash
# VÃ©rifier que le backend est accessible
curl http://localhost:5100/api/v1/health

# VÃ©rifier les CORS
docker-compose logs backend | grep CORS
```

### Base de donnÃ©es verrouillÃ©e

```bash
docker-compose down
docker-compose up -d
```
